## ================= Elastic Enterprise Search Configuration ==================
#
# NOTE: Elastic Enterprise Search comes with reasonable defaults.
#       Before adjusting the configuration, make sure you understand what you
#       are trying to accomplish and the consequences.
#
# NOTE: For passwords, the use of environment variables is encouraged
#       to keep values from being written to disk, e.g.
#       elasticsearch.password: ${ELASTICSEARCH_PASSWORD:changeme}
#
# ---------------------------------- Secrets ----------------------------------
#
# Encryption keys to protect your application secrets. This field is required.
#
secret_management.encryption_keys: [40bbd29056dd1f454ffea3834ffdeb320b33e6c19e1ff121ca40ab6714abb306]
#
# ------------------------------- Elasticsearch -------------------------------
#
# Enterprise Search needs one-time permission to alter Elasticsearch settings.
# Ensure the Elasticsearch settings are correct, then set the following to
# true. Or, adjust Elasticsearch's config/elasticsearch.yml instead.
# See README.md for more details.
#
allow_es_settings_modification: true
#
# Elasticsearch full cluster URL:
#
elasticsearch.host: https://elk-single-node:9200
#
# Elasticsearch credentials:
#
elasticsearch.username: elastic
elasticsearch.password: Ucsc@1234
#
# Alternatively, use a service token to connect to Elasticsearch:
#
# elasticsearch.service_account_token: insert-token-here
#
# Elasticsearch custom HTTP headers to add to each request:
#
#elasticsearch.headers:
#  X-My-Header: Contents of the header
#
# Elasticsearch SSL settings:
#
# SSL communication with Elasticsearch enabled or not.
#
elasticsearch.ssl.enabled: true
#
# Path to client certificate file to use for client-side validation from Elasticsearch.
#
elasticsearch.ssl.certificate: /enterprise-search-certs/certs/elastic/elastic.crt
#
# Path to the keystore that contains Certificate Authorities for Elasticsearch SSL certificate.
#
elasticsearch.ssl.certificate_authority: /enterprise-search-certs/certs/ca/ca.crt
#
# Path to the key file for the client certificate.
#
elasticsearch.ssl.key: /enterprise-search-certs/certs/elastic/elastic.key
#
# Passphrase for the above key file.
#
#elasticsearch.ssl.key_passphrase:
#
# true to verify SSL certificate from Elasticsearch, false otherwise.
#
#elasticsearch.ssl.verify: true
#
# Elasticsearch startup retry:
#
#elasticsearch.startup_retry.enabled: true
#elasticsearch.startup_retry.interval: 5 # seconds
#elasticsearch.startup_retry.fail_after: 600 # seconds
#
# ---------------------------------- Kibana -----------------------------------
#
# Define the URL at which Enterprise Search can reach Kibana.
# Defaults to http://localhost:5601 for testing purposes.
#
kibana.host: https://elk-single-node:5601
#
# Define the exposed URL at which users can reach Kibana.
# Defaults to the kibana.host setting value if not set.
#
#kibana.external_url:
#
# Custom HTTP headers to add to requests made to Kibana from Enterprise Search.
#
#kibana.headers:
#  X-My-Header: Contents of the header
#
# Kibana startup retry:
#
#kibana.startup_retry.enabled: false
#kibana.startup_retry.interval: 5 # seconds
#kibana.startup_retry.fail_after: 600 # seconds
#
# ------------------------------- Hosting & Network ---------------------------
#
# Define the exposed URL at which users will reach Enterprise Search.
# Defaults to localhost:3002 for testing purposes.
# Most cases will use one of:
#
# * An IP: http://255.255.255.255
# * A FQDN: http://example.com
# * Shortname defined via /etc/hosts: http://ent-search.search
#
ent_search.external_url: http://elk-single-node:3002
#
# Web application listen_host and listen_port.
# Your application will run on this host and port.
#
# * ent_search.listen_host: Must be a valid IPv4 or IPv6 address.
# * ent_search.listen_port: Must be a valid port number (1-65535).
#
ent_search.listen_host: 0.0.0.0
ent_search.listen_port: 3002
#
# ---------------------------------- Limits -----------------------------------
#
# Configurable limits for Enterprise Search.
# NOTE: Overriding the default limits can impact performance negatively.
#       Also, changing a limit here does not actually guarantee that
#       Enterprise Search will work as expected as related Elasticsearch limits
#       can be exceeded.
#
#### Workplace Search
#
# Configure the maximum allowed document size for a content source.
#
#workplace_search.content_source.document_size.limit: 100kb
#
# Configure how many fields a content source can have.
# NOTE: The Elasticsearch/Lucene setting `indices.query.bool.max_clause_count`
# might also need to be adjusted if "Max clause count exceeded" errors start
# occurring. See more here: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-settings.html
#
#workplace_search.content_source.total_fields.limit: 64
#
#
# Configure whether or not workplace search can run synchronization jobs.
# If this is set to `false`, no syncs will run.
# Default is `true`.
#workplace_search.content_source.sync.enabled: true
#
# Configure how many errors to tolerate in a sync job.
# If the job encounters more total errors than this value, the job will fail.
# NOTE: this only applies to errors tied to individual documents.
#
#workplace_search.content_source.sync.max_errors: 1000
#
# Configure how many errors in a row to tolerate in a sync job.
# If the job encounters more errors in a row than this value, the job will fail.
# NOTE: this only applies to errors tied to individual documents.
#
#workplace_search.content_source.sync.max_consecutive_errors: 10
#
# Configure the ratio of <errored documents> / <total documents> to tolerate in a sync job
# or in a rolling window (see `workplace_search.content_source.sync.error_ratio_window_size`).
# If the job encounters an error ratio greater than this value in a given window, or overall
# at the end of the job, the job will fail.
# NOTE: this only applies to errors tied to individual documents.
#
#workplace_search.content_source.sync.max_error_ratio: 0.15
#
# Configure how large of a window to consider when calculating an error ratio
# (see `workplace_search.content_source.sync.max_error_ratio`).
#
#workplace_search.content_source.sync.error_ratio_window_size: 100
#
# Configure whether or not a content source should generate thumbnails for the documents
# it syncs. Not all file types/sizes/content or Content Sources support thumbnail generation,
# even if this is enabled.
#
#workplace_search.content_source.sync.thumbnails.enabled: true
#
# Configure how many indexing rules a content source can have.
#
#workplace_search.content_source.indexing.rules.limit: 100
#
# Configure the refresh interval for full sync job (in ISO 8601 Duration format).
#
#workplace_search.content_source.sync.refresh_interval.full: P3D
#
# Configure the refresh interval for incremental sync job (in ISO 8601 Duration format).
#
#workplace_search.content_source.sync.refresh_interval.incremental: PT2H
#
# Configure the refresh interval for delete sync job (in ISO 8601 Duration format).
#
#workplace_search.content_source.sync.refresh_interval.delete: PT6H
#
# Configure the refresh interval for permissions sync job (in ISO 8601 Duration format).
#
#workplace_search.content_source.sync.refresh_interval.permissions: PT5M
#
# Configure whether or not Salesforce and Salesforce Sandbox connectors should sync Cases.
#
#workplace_search.content_source.salesforce.enable_cases: true
#
# Configure total number of synonym sets a Workplace Search instance can have.
#
#workplace_search.synonyms.sets.limit: 256
#
# Configure total number of terms an individual synonym set can have.
#
#workplace_search.synonyms.terms_per_set.limit: 32
#
# Configure the query timeout (in milliseconds) for remote sources via the Search API.
#
#workplace_search.remote_sources.query_timeout: 10000
#
# Configure whether to allow localhost URLs as base URLs in content sources (by default, they are not allowed).
# NOTE: This option is only valid for on-prem installations, not cloud.
#
#workplace_search.content_source.localhost_base_urls.enabled: false
#
#### App Search
#
# Configure the maximum allowed document size.
#
#app_search.engine.document_size.limit: 100kb
#
# Configure how many fields an engine can have.
# NOTE: The Elasticsearch/Lucene setting `indices.query.bool.max_clause_count`
# might also need to be adjusted if "Max clause count exceeded" errors start
# occurring. See more here: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-settings.html
#
#app_search.engine.total_fields.limit: 64
#
# Configure how many source engines a meta engine can have.
#
#app_search.engine.source_engines_per_meta_engine.limit: 15
#
# Configure how many facet values can be returned by a search.
#
#app_search.engine.total_facet_values_returned.limit: 250
#
# Configure how big full-text queries are allowed.
# NOTE: The Elasticsearch/Lucene setting `indices.query.bool.max_clause_count`
# might also need to be adjusted if "Max clause count exceeded" errors start
# occurring. See more here: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-settings.html
#
#app_search.engine.query.limit: 128
#
# Configure total number of synonym sets an engine can have.
#
#app_search.engine.synonyms.sets.limit: 256
#
# Configure total number of terms a synonym set can have.
#
#app_search.engine.synonyms.terms_per_set.limit: 32
#
# Configure how many analytics tags can be associated with a single query or clickthrough.
#
#app_search.engine.analytics.total_tags.limit: 16
#
# ---------------------------------- Workers ----------------------------------
#
# Configure the number of worker threads.
#
#worker.threads: 1
#
# ----------------------------------- APIs ------------------------------------
#
# Set to true hide product version information from API responses.
#
#hide_version_info: false
#
# ---------------------------------- Mailer -----------------------------------
#
# Connect Enterprise Search to a mailer.
# Docs: https://www.elastic.co/guide/en/workplace-search/current/workplace-search-smtp-mailer.html
#
#email.account.enabled: false
#email.account.smtp.auth: plain
#email.account.smtp.starttls.enable: false
#email.account.smtp.host: 127.0.0.1
#email.account.smtp.port: 25
#email.account.smtp.user:
#email.account.smtp.password:
#email.account.email_defaults.from:
#
# ---------------------------------- Logging ----------------------------------
#
# Choose your log export path.
#
log_directory: /var/log/enterprise-search
#
# Log level can be: debug, info, warn, error, fatal, or unknown.
#
#log_level: info
#
# Log format can be: default, json
#
#log_format: default
#
# Choose your Filebeat logs export path.
#
filebeat_log_directory: /var/log/enterprise-search
#
# Enable logging app logs to stdout (enabled by default).
#
#enable_stdout_app_logging: true
#
# The number of files to keep on disk when rotating logs. When set to 0, no
# rotation will take place.
#
#log_rotation.keep_files: 7
#
# The maximum file size in bytes before rotating the log file. If
# log_rotation.keep_files is set to 0, no rotation will take place and there
# will be no size limit for the singular log file.
#
#log_rotation.rotate_every_bytes: 1048576 # 1 MiB
#
# ------------------------------- Audit Logging -------------------------------
#
# Choose your Audit log export path. Defaults to the value of log_directory.
#
#audit_log_directory: log
#
# The number of files to keep on disk when rotating audit logs. When set to 0,
# no rotation will take place.
#
#audit_log_rotation.keep_files: 7
#
# The maximum file size in bytes before rotating the audit log file. If
# audit_log_rotation.keep_files is set to 0, no rotation will take place and
# there will be no size limit for the singular log file.
#
#audit_log_rotation.rotate_every_bytes: 1048576 # 1 MiB
#
# ------------------------------- Event Logging -------------------------------
#
# Enable or disable indexing of Elasticsearch App Search Crawler Event logs.
# These are enabled by default.
# Disabling these will impact dashboards and analytics.
#
#connector.crawler.logging.events.enabled: true
#
# ---------------------------------- TLS/SSL ----------------------------------
#
# Configure TLS/SSL encryption.
#
#ent_search.ssl.enabled: false
#ent_search.ssl.keystore.path:
#ent_search.ssl.keystore.password:
#ent_search.ssl.keystore.key_password:
#ent_search.ssl.redirect_http_from_port:
#
# ---------------------------------- Session ----------------------------------
#
# Set a session key to persist user sessions through process restarts.
#
#secret_session_key:
secret_session_key: ec5996e643daad0e550f07401518d09287dd8146ea8300c0194b6d674540188e590379a57828e76999187a1a11fbe788490368df8f88de76fbc6e755ae04c8f9
#
# ---------------------------- APM Instrumentation -----------------------------
#
# Enable Elastic APM agent within Enterprise Search
#
#apm.enabled: true
#
# Set the custom APM Server URL
#
#apm.server_url: 'http://localhost:8200'
#
# Set the APM authentication token (use if APM Server requires a secret token)
#
#apm.secret_token: 'your-token-here'
#
# Override the APM service name
# Allowed characters: a-z, A-Z, 0-9, -, _ and space
#
#apm.service_name: 'Enterprise Search'
#
# Override the APM service environment
#
#apm.environment: 'production'
#
# Optional global labels added to all events
#
#apm.global_labels:
#  custom_label_key: 'label value'
#
# Optional name for the service node
#
#apm.service_node_name: 'Enterprise Search Service Node'
#
# --------------------------------- Monitoring ---------------------------------
#
# Enable automatic monitoring metrics reporting to Elasticsearch via metricbeat
#
#monitoring.reporting_enabled: false
#
# Configure metrics reporting frequency
#
# Note: This setting should be aligned with `monitoring.ui.min_interval_seconds`
# setting in Kibana, or Stack Monitoring dashboards for Enterprise Search may
# have gaps in graphs on high metric resolutions.
#
#monitoring.reporting_period: 10s
#
# Configure metricsets to be reported to Elasticsearch
#
#monitoring.metricsets: ['health', 'stats']
#
# Override the index name prefix used to index Enterprise Search metrics
# Note: The index will have ILM enabled and will be managed by Enterprise Search
#
#monitoring.index_prefix: metricbeat-ent-search
#
# ----------------------------- Diagnostics report ----------------------------
#
# Path where diagnostic reports will be generated.
#
#diagnostic_report_directory: diagnostics
#
# ------------------------------ Crawler ------------------------------
# This documentation is only for Crawler. All configuration prefixed with `connector.crawler.*` belong to Crawler.
# For App Search Crawler documentation, see the section below titled "App Search Crawler".
#
# The User-Agent HTTP Header used for the Crawler.
#
connector.crawler.http.user_agent: Elastic-Crawler (8.11.3)
#
# The user agent platform used for the Crawler with identifying information:
# https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent#syntax
# Note: this value will be added as a suffix to `crawler.http.user_agent` and
# used as the final User-Agent Header.
#
#connector.crawler.http.user_agent_platform:
#
# The number of parallel crawls allowed per instance of Enterprise Search.
# By default, it is set to 2x the number of available logical CPU cores.
# Note: On Intel CPUs, the default value is 4x the number of physical CPU cores
# due to hyper-threading (https://en.wikipedia.org/wiki/Hyper-threading).
#
#connector.crawler.workers.pool_size.limit: N
#
# -------------------------
# Per-crawl Resource Limits
# -------------------------
#
# These limits guard against infinite loops and other traps common to
# production web crawlers.
#
# If your crawler is hitting these limits, try changing your crawl rules
# or the content you're crawling. Adjust these limits as a last resort.
#
#
# The maximum duration of a crawl, in seconds. Beyond this limit, the
# crawler will stop, abandoning all remaining URLs in the crawl queue.
#
#connector.crawler.crawl.max_duration.limit: 86400 # seconds
#
# The maximum number of sequential pages the crawler will traverse starting
# from the given set of entry points. Beyond this limit, the crawler will stop
# discovering new links.
#
#connector.crawler.crawl.max_crawl_depth.limit: 10
#
# The maximum number of characters within each URL to crawl. The crawler
# will skip URLs that exceed this length.
#
#connector.crawler.crawl.max_url_length.limit: 2048
#
# The maximum number of segments within the path of each URL to crawl. The
# crawler will skip URLs whose paths exceed this length.
# Example: The path '/a/b/c/d' has 4 segments.
#
#connector.crawler.crawl.max_url_segments.limit: 16
#
# The maximum number of query parameters within each URL to crawl. The
# crawler will skip URLs that exceed this length.
# Example: The query string in '/a?b=c&d=e' has 2 query parameters.
#
#connector.crawler.crawl.max_url_params.limit: 32
#
# The maximum number of unique URLs the crawler will index during a single crawl.
# Beyond this limit, the crawler will stop.
#
#connector.crawler.crawl.max_unique_url_count.limit: 100000
#
# -------------------------
# Advanced Per-crawl Limits
# -------------------------
#
# The number of parallel threads to use for each crawl.
# The main effect from increasing this value will be an increased throughput
# of the crawler at the expense of higher CPU load on Enterprise Search and
# Elasticsearch instances as well as higher load on the website being crawled.
#
#connector.crawler.crawl.threads.limit: 10
#
# The maximum size of the crawl frontier - the list of URLs the crawler needs to visit.
# The list is stored in Elasticsearch, so the limit could be increased as long
# as the Elasticsearch cluster has enough resources (disk space) to hold the queue index.
#
#connector.crawler.crawl.url_queue.url_count.limit: 100000
#
# ---------------------------
# Per-Request Timeout Limits
# ---------------------------
#
# The maximum period to wait until abortion of the request, when a connection is being initiated.
#
#connector.crawler.http.connection_timeout: 10 # seconds
#
# The maximum period of inactivity between two data packets, before the request is aborted.
#
#connector.crawler.http.read_timeout: 10 # seconds
#
# The maximum period of the entire request, before the request is aborted.
#
#connector.crawler.http.request_timeout: 60 # seconds
#
# ---------------------------
# Per-Request Resource Limits
# ---------------------------
#
# The maximum size of an HTTP response (in bytes) supported by the crawler.
#
#connector.crawler.http.response_size.limit: 10485760
#
# The maximum number of HTTP redirects before a request is failed.
#
#connector.crawler.http.redirects.limit: 10
#
# ----------------------------------
# Content Extraction Resource Limits
# ----------------------------------
#
# The maximum size (in bytes) of some fields extracted from crawled pages
#
#connector.crawler.extraction.title_size.limit: 1024
#connector.crawler.extraction.body_size.limit: 5242880
#connector.crawler.extraction.keywords_size.limit: 512
#connector.crawler.extraction.description_size.limit: 1024
#
# The maximum number of links extracted from each page for further crawling
#
#connector.crawler.extraction.extracted_links_count.limit: 1000
#
# The maximum number of links extracted from each page and indexed in a document
#
#connector.crawler.extraction.indexed_links_count.limit: 25
#
# The maximum number of HTML headers to be extracted from each page
#
#connector.crawler.extraction.headings_count.limit: 25
#
# Default document fields used to compare documents during de-duplication
#
#connector.crawler.extraction.default_deduplication_fields: ['title', 'body_content', 'meta_keywords', 'meta_description', 'links', 'headings']
#
# ----------------------------------
# File Content Extraction
# ----------------------------------
#
# Enable extraction of non-HTML files found during a crawl
#
#connector.crawler.content_extraction.enabled: false
#
# Extract files with the following MIME types
#
#connector.crawler.content_extraction.mime_types: []
#
# ------------------------------
# Crawler HTTP Auth Controls
# ------------------------------
#
# Allow _authenticated_ crawling of non-HTTPS URLs.
# WARNING: Enabling this setting could expose your Authorization headers to a
# man-in-the-middle attack and should never be used in production deployments.
# See https://en.wikipedia.org/wiki/Man-in-the-middle_attack for more details.
#
#connector.crawler.security.auth.allow_http: false
#
# ------------------------------
# Crawler HTTP Security Controls
# ------------------------------
#
# A list of custom SSL Certificate Authority certificates to be used for all
# connections made by the crawler to your websites. These certificates are added
# to the standard list of CA certificates trusted by the JVM.
#
# Please note: Each item in this list could be a file name of a certificate in
# PEM format or a PEM-formatted certificate as a string.
#
#connector.crawler.security.ssl.certificate_authorities: []
#
# Control SSL verification mode used by the crawler:
#  * full - validate both the SSL certificate and the hostname presented by the
#            server (this is the default and the recommended value)
#  * certificate - only validate the SSL certificate presented by the server
#  * none - disable SSL validation completely (this is very dangerous and
#           should never be used in production deployments).
#
#connector.crawler.security.ssl.verification_mode: full
#
# -----------------------------
# Crawler DNS Security Controls
# -----------------------------
#
# WARNING: The settings in this section could make your deployment vulnerable to
# SSRF attacks (especially in cloud environments) from the owners of any domains
# you crawl. Do not enable any of the settings here unless you fully control DNS
# domains you access with the crawler.
#
# See https://owasp.org/www-community/attacks/Server_Side_Request_Forgery for
# more details on the SSRF attack and the risks associated with it.
#
# Allow crawler to access the localhost (127.0.0.0/8 IP namespace)
#
#connector.crawler.security.dns.allow_loopback_access: false
#
# Allow crawler to access the private IP space: link-local, network-local addresses, etc
# (see https://en.wikipedia.org/wiki/Reserved_IP_addresses#IPv4 for more details)
#
#connector.crawler.security.dns.allow_private_networks_access: false
#
# ---------------------------
# Crawler HTTP Proxy Settings
# ---------------------------
#
# If you need the Crawler to send HTTP requests through a proxy,
# you can configure it below.
#
# Please note:
#  * Only HTTP and HTTPS proxies are supported at the moment.
#  * Your proxy connections are subject to the DNS security controls described
#    above (if your proxy server is running on a private or a loopback address,
#    you will need to explicitly allow the crawler to connect to it).
#
#connector.crawler.http.proxy.host:
#connector.crawler.http.proxy.port: 8080
#
# Protocol to be used for connecting to the proxy: http (default) or https
#
#connector.crawler.http.proxy.protocol: http
#
# Basic HTTP credentials to be used for connecting to the proxy.
# NOTE: Proxy authentication is only supported for Elasticsearch clusters
# with at least a platinum (or trial) license.
#
#connector.crawler.http.proxy.username:
#connector.crawler.http.proxy.password:
#
# -----------------------
# Advanced Crawler Tuning
# -----------------------
#
# Enable/disable HTTP content (gzip/deflate) compression in Crawler requests
#
#connector.crawler.http.compression.enabled: true
#
# Default encoding used for responses that do not specify a charset
#
#connector.crawler.http.default_encoding: UTF-8
#
# Enable/disable performing HEAD before GET requests in Crawler requests
# Fetching a HEAD request first can allow for faster crawls on websites with
# many redirects or unsupported content types
#
#connector.crawler.http.head_requests.enabled: false
#
# ------------------------------ App Search Crawler ------------------------------
# This documentation is only for App Search Crawler. All configuration prefixed with
# only `crawler.*` belong to App Search Crawler.
# For Crawler documentation, see the section above titled "Crawler".
#
# The User-Agent HTTP Header used for the App Search Crawler.
#
#crawler.http.user_agent: Elastic-Crawler (<crawler_version_number>)
#
# The user agent platform used for the App Search Crawler with identifying information:
# https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent#syntax
# Note: this value will be added as a suffix to `crawler.http.user_agent` and
# used as the final User-Agent Header.
#
#crawler.http.user_agent_platform:
#
# The number of parallel crawls allowed per instance of Enterprise Search.
# By default, it is set to 2x the number of available logical CPU cores.
# Note: On Intel CPUs, the default value is 4x the number of physical CPU cores
# due to hyper-threading (https://en.wikipedia.org/wiki/Hyper-threading).
#
#crawler.workers.pool_size.limit: N
#
# -------------------------
# Per-crawl Resource Limits
# -------------------------
#
# These limits guard against infinite loops and other traps common to
# production web crawlers.
#
# If your app search crawler is hitting these limits, try changing your crawl rules
# or the content you're crawling. Adjust these limits as a last resort.
#
#
# The maximum duration of a crawl, in seconds. Beyond this limit, the
# app search crawler will stop, abandoning all remaining URLs in the crawl queue.
#
#crawler.crawl.max_duration.limit: 86400 # seconds
#
# The maximum number of sequential pages the app search crawler will traverse starting
# from the given set of entry points. Beyond this limit, the app search crawler will stop
# discovering new links.
#
#crawler.crawl.max_crawl_depth.limit: 10
#
# The maximum number of characters within each URL to crawl. The app search crawler
# will skip URLs that exceed this length.
#
#crawler.crawl.max_url_length.limit: 2048
#
# The maximum number of segments within the path of each URL to crawl. The
# app search crawler will skip URLs whose paths exceed this length.
# Example: The path '/a/b/c/d' has 4 segments.
#
#crawler.crawl.max_url_segments.limit: 16
#
# The maximum number of query parameters within each URL to crawl. The
# app search crawler will skip URLs that exceed this length.
# Example: The query string in '/a?b=c&d=e' has 2 query parameters.
#
#crawler.crawl.max_url_params.limit: 32
#
# The maximum number of unique URLs the app search crawler will index during a single crawl.
# Beyond this limit, the app search crawler will stop.
#
#crawler.crawl.max_unique_url_count.limit: 100000
#
# -------------------------
# Advanced Per-crawl Limits
# -------------------------
#
# The number of parallel threads to use for each crawl.
# The main effect from increasing this value will be an increased throughput
# of the app search crawler at the expense of higher CPU load on Enterprise Search and
# Elasticsearch instances as well as higher load on the website being crawled.
#
#crawler.crawl.threads.limit: 10
#
# The maximum size of the crawl frontier - the list of URLs the app search crawler needs to visit.
# The list is stored in Elasticsearch, so the limit could be increased as long
# as the Elasticsearch cluster has enough resources (disk space) to hold the queue index.
#
#crawler.crawl.url_queue.url_count.limit: 100000
#
# ---------------------------
# Per-Request Timeout Limits
# ---------------------------
#
# The maximum period to wait until abortion of the request, when a connection is being initiated.
#
#crawler.http.connection_timeout: 10 # seconds
#
# The maximum period of inactivity between two data packets, before the request is aborted.
#
#crawler.http.read_timeout: 10 # seconds
#
# The maximum period of the entire request, before the request is aborted.
#
#crawler.http.request_timeout: 60 # seconds
#
# ---------------------------
# Per-Request Resource Limits
# ---------------------------
#
# The maximum size of an HTTP response (in bytes) supported by the app search crawler.
#
#crawler.http.response_size.limit: 10485760
#
# The maximum number of HTTP redirects before a request is failed.
#
#crawler.http.redirects.limit: 10
#
# ----------------------------------
# Content Extraction Resource Limits
# ----------------------------------
#
# The maximum size (in bytes) of some fields extracted from crawled pages
#
#crawler.extraction.title_size.limit: 1024
#crawler.extraction.body_size.limit: 5242880
#crawler.extraction.keywords_size.limit: 512
#crawler.extraction.description_size.limit: 1024
#
# The maximum number of links extracted from each page for further crawling
#
#crawler.extraction.extracted_links_count.limit: 1000
#
# The maximum number of links extracted from each page and indexed in a document
#
#crawler.extraction.indexed_links_count.limit: 25
#
# The maximum number of HTML headers to be extracted from each page
#
#crawler.extraction.headings_count.limit: 25
#
# Default document fields used to compare documents during de-duplication
#
#crawler.extraction.default_deduplication_fields: ['title', 'body_content', 'meta_keywords', 'meta_description', 'links', 'headings']
#
# ----------------------------------
# File Content Extraction
# ----------------------------------
#
# Enable extraction of non-HTML files found during a crawl
#
#crawler.content_extraction.enabled: false
#
# Extract files with the following MIME types
#
#crawler.content_extraction.mime_types: []
#
# ------------------------------
# App Search Crawler HTTP Auth Controls
# ------------------------------
#
# Allow _authenticated_ crawling of non-HTTPS URLs.
# WARNING: Enabling this setting could expose your Authorization headers to a
# man-in-the-middle attack and should never be used in production deployments.
# See https://en.wikipedia.org/wiki/Man-in-the-middle_attack for more details.
#
#crawler.security.auth.allow_http: false
#
# ------------------------------
# App Search Crawler HTTP Security Controls
# ------------------------------
#
# A list of custom SSL Certificate Authority certificates to be used for all
# connections made by the app search crawler to your websites. These certificates are added
# to the standard list of CA certificates trusted by the JVM.
#
# Please note: Each item in this list could be a file name of a certificate in
# PEM format or a PEM-formatted certificate as a string.
#
#crawler.security.ssl.certificate_authorities: []
#
# Control SSL verification mode used by the app search crawler:
#  * full - validate both the SSL certificate and the hostname presented by the
#            server (this is the default and the recommended value)
#  * certificate - only validate the SSL certificate presented by the server
#  * none - disable SSL validation completely (this is very dangerous and
#           should never be used in production deployments).
#
#crawler.security.ssl.verification_mode: full
#
# -----------------------------
# App Search Crawler DNS Security Controls
# -----------------------------
#
# WARNING: The settings in this section could make your deployment vulnerable to
# SSRF attacks (especially in cloud environments) from the owners of any domains
# you crawl. Do not enable any of the settings here unless you fully control DNS
# domains you access with the app search crawler.
#
# See https://owasp.org/www-community/attacks/Server_Side_Request_Forgery for
# more details on the SSRF attack and the risks associated with it.
#
# Allow app search crawler to access the localhost (127.0.0.0/8 IP namespace)
#
#crawler.security.dns.allow_loopback_access: false
#
# Allow app search crawler to access the private IP space: link-local, network-local addresses, etc
# (see https://en.wikipedia.org/wiki/Reserved_IP_addresses#IPv4 for more details)
#
#crawler.security.dns.allow_private_networks_access: false
#
# ---------------------------
# App Search Crawler HTTP Proxy Settings
# ---------------------------
#
# If you need the App Search Crawler to send HTTP requests through a proxy,
# you can configure it below.
#
# Please note:
#  * Only HTTP and HTTPS proxies are supported at the moment.
#  * Your proxy connections are subject to the DNS security controls described
#    above (if your proxy server is running on a private or a loopback address,
#    you will need to explicitly allow the app search crawler to connect to it).
#
#crawler.http.proxy.host:
#crawler.http.proxy.port: 8080
#
# Protocol to be used for connecting to the proxy: http (default) or https
#
#crawler.http.proxy.protocol: http
#
# Basic HTTP credentials to be used for connecting to the proxy.
# NOTE: Proxy authentication is only supported for Elasticsearch clusters
# with at least a platinum (or trial) license.
#
#crawler.http.proxy.username:
#crawler.http.proxy.password:
#
# -----------------------
# Advanced App Search Crawler Tuning
# -----------------------
#
# Enable/disable HTTP content (gzip/deflate) compression in App Search Crawler requests
#
#crawler.http.compression.enabled: true
#
# Default encoding used for responses that do not specify a charset
#
#crawler.http.default_encoding: UTF-8
#
# ------------------------------ Read-only mode -------------------------------
#
# If true, pending migrations can be executed without enabling read-only mode.
# Proceeding with migrations while indices are allowing writes can have
# unintended consequences. Use at your own risk, should not be set to true when
# upgrading a production instance with ongoing traffic.
#
#skip_read_only_check: false
#
